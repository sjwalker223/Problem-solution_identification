{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify problems and solutions in scientific text.\n",
    "\n",
    "## The problem\n",
    "\n",
    "I work in a fast-moving field where a large number of scientific papers are published every week. I don't have the time to read every paper that's published in my field - so how do I choose which ones to focus on? Right now, I skim their abstracts and decide which look interesting - but this takes a long time, and sometimes the abstract doesn't describe a paper well. Instead, there are two things I really want to know when deciding whether to read a paper:\n",
    "\n",
    "1) What are the **problems** this paper is trying to address?\n",
    "2) What are the **solutions** the authors propose?\n",
    "\n",
    "## The solution\n",
    "\n",
    "I want to train models that can pick out just those sentences in the text of a scientific paper that describe problems and proposed/identified solutions. That way, I can screen just these sentences, and decide whether these are problems/solutions that interest me.\n",
    "\n",
    "## The approach\n",
    "\n",
    "#### General approach\n",
    "I will make use of a dataset published in [1] and available at [2] (see References at end of notebook). This dataset contains sentences extracted from scientific articles, and hand-annotated with the presence and location of problem and solution phrases within the sentence. I will use this dataset to train two classes of models:\n",
    "1) **Problem sentence identifier** - that will identify sentences that contain a problem phrase\n",
    "2) **Solution sentence identifier** - that will identify sentences that contain a solution phrase\n",
    "\n",
    "#### Previous work\n",
    "This original paper for this dataset [1] describes an approach to train a number of different machine learning classification models to predict whether a sentence contains a problem and/or a solution. These include logistic regression, Naive Bayes, and Support Vector Machine models. The authors achieve reasonable performance, namely 82.3% and 79.7% accuracy using their best-performing models to identify problems and solutions, respectively (SVMs using both bag-of-words and a number of syntactic features as predictors). A more recent paper [3] extended these results to implement deep neural network models to perform the same classification task. The best-performing classifiers were convolutional neural networks, which achieved 90% and 86% accuracy for problems and solutions, respectively.\n",
    "\n",
    "#### My approach\n",
    "I will first train a \"traditional\" ML model to perform the problem/non-problem and solution/non-solution classification tasks. Specifically, I will use a **Naive Bayes** model, because such models generally perform well on text data, and are easy to train.\n",
    "\n",
    "Then, I will try a different approach using more recent advances in neural network architectures. I will take advantage of Google's open source **BERT** model, which is pre-trained on English natural texts. I will fine-tune this model (add a single-unit classification layer and run several training runs) to create a **Transformer**-based classifier that can identify problem sentences, and similary for solutions.\n",
    "\n",
    "## Making the model usable\n",
    "\n",
    "To demonstrate the utility of this model, I will feed it with a scientific paper, and ask it to identify the problems that the paper is trying to address, and the solutions that the authors propose.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "A simple Naive Bayes classifier did an OK job of classifying sentences as containing problems or solutions. However, fine-tuning a BERT model resulted in a much higher performance on test data. In fact, the performance of both the problem and solution BERT classifiers **exceeded the best published performance results** on this dataset.\n",
    "\n",
    "##### Therefore, I have a developed a model that can very accuractely pick out problems and solutions from a scientific text, which will help me to parse the scientific literature much more efficiently!\n",
    "\n",
    "In the future, I'd love to ftrain a Mamba model to perform this task, to see whether it can outperform my BERT models.\n",
    "\n",
    "## Structure of this notebook\n",
    "1. **Setup**\n",
    "    - package imports\n",
    "2. **Data import**\n",
    "    - import the training and test datasets\n",
    "3. **Naive Bayes classifiers**\n",
    "    - Sentence preprocessing\n",
    "    - Sentence embedding and padding\n",
    "    - Model training and validation\n",
    "    - Adding parameters\n",
    "4. **BERT classifiers**\n",
    "    - Sentence preprocessing\n",
    "    - Model initialization\n",
    "    - Training and validation\n",
    "5. **Model comparison**\n",
    "    - Testing both models on a held-out test dataset\n",
    "6. **Model usage on full article**\n",
    "\n",
    "## References\n",
    "[1] - Heffernan, Kevin, and Simone Teufel. \"Identifying problems and solutions in scientific text.\" Scientometrics 116 (2018): 1367-1382.\n",
    "\n",
    "[2] - https://github.com/kevinheffernan/problem-solving-in-scientific-text/blob/main/dataset.tar.gz\n",
    "\n",
    "[3] - Mishra, Rohit Bhuvaneshwar, and Hongbing Jiang. \"Classification of Problem and Solution Strings in Scientific Texts: Evaluation of the Effectiveness of Machine Learning Classifiers and Deep Neural Networks.\" Applied Sciences 11.21 (2021): 9997."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sjwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\sjwal\\dw-task\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import pickle\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training and test datasets\n",
    "# (The test dataset will only be used at the end for comparing models)\n",
    "\n",
    "training_file = 'training_docs_joint.jsonlines'\n",
    "test_file = 'test_docs_joint.jsonlines'\n",
    "\n",
    "def load_data_file(dataset_file):\n",
    "    \"\"\"Load a jsonl file and put in dataframe\n",
    "    Args: dataset_file (str)\n",
    "    Outputs: dataset (dataframe)\n",
    "    \"\"\"\n",
    "    # Load each line\n",
    "    data = []\n",
    "    with open(dataset_file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Put in dataframe\n",
    "    dataset = pd.DataFrame(data[0])\n",
    "    for doc in data[1:]:\n",
    "        dataset = pd.concat([dataset, pd.DataFrame(doc)])\n",
    "    dataset.index = range(len(dataset))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = load_data_file(training_file)\n",
    "test_dataset = load_data_file(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of this dataset:\n",
    "\n",
    "There are 4800 sentences in this dataset. For each piece of text, there are 5 sentences.\n",
    "\n",
    "Each sentence is tokenized in `sentences`.\n",
    "\n",
    "For each sentence, `ners` defines problem and solution statements (phrases within a sentence that describe problems or solutions). Specifically, it contains the indices of tokens defining several aspects of a problem statement:\n",
    "- `Signal` - a word or short series of words indicating a problem\n",
    "- `Root` - the cause or source of the problem (and whether this comes before (RIGHT) or after (LEFT) the signal)\n",
    "- `Condition` - conditional or restrictive comments for the problem statement (and whether this comes before (RIGHT) or after (LEFT) the signal)\n",
    "- `Complement` - more information about the problem, or an object pointed to by the problem statement  (and whether this comes before (RIGHT) or after (LEFT) the signal)\n",
    "\n",
    "For solutions, only the start and end indices of the solution statement are given.\n",
    "\n",
    "I want to train a model to predict whether a sentence contains a problem and/or a solution. So, let's count up the number of problems/solutions in each sentence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['problem'] = 0\n",
    "dataset['problem'] = dataset['ners'].apply(lambda x: sum([y.count('SIGNAL') for y in x]))\n",
    "dataset['solution'] = 0\n",
    "dataset['solution'] = dataset['ners'].apply(lambda x: sum([y.count('SOLUTION') for y in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any sentences that contain both a problem and a solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>ners</th>\n",
       "      <th>sentences</th>\n",
       "      <th>problem</th>\n",
       "      <th>solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>train_extract_20</td>\n",
       "      <td>[[2, 2, ROOT-RIGHT], [3, 3, SIGNAL], [7, 18, S...</td>\n",
       "      <td>[To, solve, this, problem, ,, we, propose, a, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>train_extract_45</td>\n",
       "      <td>[[6, 6, ROOT-RIGHT], [7, 7, SIGNAL], [9, 34, S...</td>\n",
       "      <td>[In, this, paper, ,, we, handle, this, problem...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>train_extract_61</td>\n",
       "      <td>[[3, 5, ROOT-RIGHT], [6, 6, SIGNAL], [10, 21, ...</td>\n",
       "      <td>[To, address, the, above, -, mentioned, issues...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>train_extract_62</td>\n",
       "      <td>[[3, 10, SOLUTION], [13, 13, ROOT-RIGHT], [14,...</td>\n",
       "      <td>[This, paper, develops, a, general, framework,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>train_extract_77</td>\n",
       "      <td>[[15, 22, SOLUTION], [25, 25, ROOT-RIGHT], [26...</td>\n",
       "      <td>[In, a, recent, paper, (, Erk, and, Padó, ,, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>train_extract_773</td>\n",
       "      <td>[[6, 23, SOLUTION], [27, 27, SIGNAL], [29, 34,...</td>\n",
       "      <td>[In, this, paper, ,, we, discuss, a, new, auto...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>train_extract_774</td>\n",
       "      <td>[[8, 26, SOLUTION], [30, 32, ROOT-RIGHT], [33,...</td>\n",
       "      <td>[Our, goal, in, this, work, was, to, create, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4655</th>\n",
       "      <td>train_extract_776</td>\n",
       "      <td>[[2, 8, SOLUTION], [11, 11, ROOT-RIGHT], [12, ...</td>\n",
       "      <td>[We, propose, a, new, unsupervised, model, for...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4733</th>\n",
       "      <td>train_extract_789</td>\n",
       "      <td>[[6, 6, SIGNAL], [8, 11, ROOT-LEFT], [14, 22, ...</td>\n",
       "      <td>[Our, goal, is, to, address, the, weaknesses, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>train_extract_790</td>\n",
       "      <td>[[3, 11, SOLUTION], [15, 15, COMPLEMENT-RIGHT]...</td>\n",
       "      <td>[This, paper, describes, an, approach, to, fea...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                doc_key                                               ners  \\\n",
       "119    train_extract_20  [[2, 2, ROOT-RIGHT], [3, 3, SIGNAL], [7, 18, S...   \n",
       "269    train_extract_45  [[6, 6, ROOT-RIGHT], [7, 7, SIGNAL], [9, 34, S...   \n",
       "365    train_extract_61  [[3, 5, ROOT-RIGHT], [6, 6, SIGNAL], [10, 21, ...   \n",
       "371    train_extract_62  [[3, 10, SOLUTION], [13, 13, ROOT-RIGHT], [14,...   \n",
       "461    train_extract_77  [[15, 22, SOLUTION], [25, 25, ROOT-RIGHT], [26...   \n",
       "...                 ...                                                ...   \n",
       "4637  train_extract_773  [[6, 23, SOLUTION], [27, 27, SIGNAL], [29, 34,...   \n",
       "4643  train_extract_774  [[8, 26, SOLUTION], [30, 32, ROOT-RIGHT], [33,...   \n",
       "4655  train_extract_776  [[2, 8, SOLUTION], [11, 11, ROOT-RIGHT], [12, ...   \n",
       "4733  train_extract_789  [[6, 6, SIGNAL], [8, 11, ROOT-LEFT], [14, 22, ...   \n",
       "4739  train_extract_790  [[3, 11, SOLUTION], [15, 15, COMPLEMENT-RIGHT]...   \n",
       "\n",
       "                                              sentences  problem  solution  \n",
       "119   [To, solve, this, problem, ,, we, propose, a, ...        1         1  \n",
       "269   [In, this, paper, ,, we, handle, this, problem...        1         1  \n",
       "365   [To, address, the, above, -, mentioned, issues...        1         1  \n",
       "371   [This, paper, develops, a, general, framework,...        1         1  \n",
       "461   [In, a, recent, paper, (, Erk, and, Padó, ,, 2...        1         1  \n",
       "...                                                 ...      ...       ...  \n",
       "4637  [In, this, paper, ,, we, discuss, a, new, auto...        1         1  \n",
       "4643  [Our, goal, in, this, work, was, to, create, a...        1         1  \n",
       "4655  [We, propose, a, new, unsupervised, model, for...        1         1  \n",
       "4733  [Our, goal, is, to, address, the, weaknesses, ...        1         1  \n",
       "4739  [This, paper, describes, an, approach, to, fea...        1         1  \n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[(dataset['problem'] > 0) & (dataset['solution'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes - in fact, 86 of them. It probably makes most sense two train two models: one to identify whether a sentence contains a ***problem*** statement, and a second to identify whether a sentence contains a ***solution*** statement.\n",
    "\n",
    "Finally, I will reduce the problem/solution labels to 1/0 (contains a problem/solution or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['problem'].clip(upper=1, inplace=True)\n",
    "dataset['solution'].clip(upper=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As input, I will start with just an embedding of the sentence. In improving the model, I could then try adding other features (e.g. polarity, syntax, POS).\n",
    "\n",
    "What type of model could I use to predict sentences containing problems and solutions?\n",
    "- Traditional ML classification models\n",
    "    - Logistic regression\n",
    "    - Naive Bayes\n",
    "    - SVM\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "- Neural networks\n",
    "    - Recurrent NN (e.g. LSTM)\n",
    "    - Convolutional NN\n",
    "    - Transformer - could fine-tune a pre-trained transformer (e.g. BERT, SBERT)\n",
    "\n",
    "As a first pass, I will implement a **Naive Bayes** classifier, since it tends to perform well for classification of text data. I will first prepare the text data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes classifiers\n",
    "\n",
    "#### 3.1 Sentence preprocessing\n",
    "\n",
    "I will apply a number of preprocessing steps to the text before embedding. Note that the sentences are already tokenized.\n",
    "1. Lowercase\n",
    "2. Change \"'t\" to \"not\" (it's important to include \"not\", because this can change a problem to non-problem or vice versa)\n",
    "3. Lemmatize\n",
    "4. Remove all punctuation (currently remove \"?\" but may want to keep this)\n",
    "5. Remove stopwords except \"not\", \"can\".\n",
    "\n",
    "First, check whether any of the sentences in the dataset have \"n't\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126     [n't]\n",
       "130     [n't]\n",
       "506     [n't]\n",
       "1077    [n't]\n",
       "1701    [n't]\n",
       "2072    [n't]\n",
       "3088    [n't]\n",
       "3430    [n't]\n",
       "3880    [n't]\n",
       "4531    [n't]\n",
       "4750    [n't]\n",
       "Name: sentences, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nots = dataset['sentences'].apply(lambda y: [x for x in y if \"n\\'t\" in x])\n",
    "nots[nots.str.len() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer that was applied to this dataset separates \"n't\" as a separate token. So we can just replace this with \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [support, rule, extraction, sensitive, input, ...\n",
       "1       [demand, extraction, can, slow, phrase, base, ...\n",
       "2       [et, al, 2013, demonstrate, order, magnitude, ...\n",
       "3       [however, popular, translation, model, use, ``...\n",
       "4       [instead, need, pattern, match, phrase, extrac...\n",
       "                              ...                        \n",
       "4795    [also, use, datum, diacritic, show, improvemen...\n",
       "4796    [currently, text, tospeech, speech, text, appl...\n",
       "4797    [diacritization, system, restore, diacritic, s...\n",
       "4798    [also, would, greatly, benefit, nonnative, spe...\n",
       "4799    [propose, paper, statistical, approach, restor...\n",
       "Name: processed_sentence, Length: 4800, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = [x.lower() for x in sentence]\n",
    "    # Replace \"n't\" with \"not\"\n",
    "    sentence = [re.sub(r\"n\\'t\", \"not\", x) for x in sentence]\n",
    "    # Lemmatization\n",
    "    doc = Doc(nlp.vocab, sentence)\n",
    "    sentence = [token.lemma_ for token in nlp(doc)]\n",
    "    # Remove punctuation and stopwords\n",
    "    punc = string.punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = [word for word in sentence if\n",
    "                word not in stop_words and\n",
    "                word not in punc or\n",
    "                word in ['not', 'can']]\n",
    "    sentence = [re.sub('\\.', '', word) for word in sentence]\n",
    "    return sentence\n",
    "\n",
    "dataset['processed_sentence'] = dataset['sentences'].apply(preprocess_text)\n",
    "dataset['processed_sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Sentence embedding\n",
    "\n",
    "Next, I will embed the sentences so they can be used as input to the Naive Bayes model.\n",
    "\n",
    "I will use test the performance of the model using two different approaches:\n",
    "1. Vectorization using `TF-IDF`\n",
    "2. Embedding using `Word2Vec`\n",
    "\n",
    "I will implement Word2Vec using the `gensim` package. I could train a new embedding model based on the words in this corpus. However, I want the model to be able to generalize well to other datasets which might have different vocabularies. I will therefore use a pre-trained model (`google-news-300`, which has around 100 billion words).\n",
    "\n",
    "Word2Vec has the advantage of taking a word's context into account, whereas TF-IDF does not. However, the Naive Bayes model (sklearn.model.fit()) takes a 2d array as input (vectors x sentences), so it can't handle a vector for every word (vectors x words x sentences). To get around this, I will try two different approaches to making the word embeddings into a 1d array for each sentence:\n",
    "1. Take the **mean** embedding for each sentence\n",
    "2. **Flatten** (or concatenate) the embeddings of each word (so an array with dims (50,300) becomes an array with dims (,15000)).\n",
    "\n",
    "One possible extension of TF-IDF could be to include not just individual words, but also **ngrams** (e.g. bigrams, trigrams) as predictive features, since this may help to capture short phrases that indicate problems/solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "dataset['tfidf_text'] = dataset['processed_sentence'].apply(lambda text: \" \".join(set(text)))\n",
    "X_tfidf = tfidf_vect.fit_transform(dataset['tfidf_text'])\n",
    "X_tfidf = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec embedding using google-news-300 model\n",
    "\n",
    "w2v = api.load('word2vec-google-news-300')\n",
    "def embed_words(sentence):\n",
    "    sentence = np.stack([w2v[word] if word in w2v else w2v['unk'] for word in sentence])\n",
    "    return sentence\n",
    "dataset['embedded_sentence'] = dataset['processed_sentence'].apply(embed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before flattening the Word2Vec embeddings, I will pad the sentences so they all have the same length.\n",
    "\n",
    "What is the distribution of sentence lengths in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length = 98\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Maximum sentence length = {max(dataset['embedded_sentence'].str.len())}\")\n",
    "ax = sns.histplot(dataset['embedded_sentence'].str.len())\n",
    "_ = ax.axvline(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture all sentences in the dataset, I will employ a length of 50 tokens. If the padding is too long (e.g. 100 tokens), it will negatively impact accuracy when flattening the embeddings.\n",
    "\n",
    "I will try padding with embeddings = `w2v['unk']`. Alternatively, I could pad with zeros, or the mean of the embeddings of the tokens in the corpus - although because I'm not sure how representative the words in this corpus are of words in the inference corpus, the latter might not be a good approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad with unk\n",
    "\n",
    "def pad_sentences(sentence, max_len=50):\n",
    "    sentence = np.vstack([sentence, np.tile(w2v['unk'],\n",
    "                                            (max_len - len(sentence), 1))]) if len(sentence) <= max_len else sentence[:max_len]\n",
    "    return sentence\n",
    "dataset['padded_sentence'] = dataset['embedded_sentence'].apply(pad_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will turn the word embeddings into 1d arrays by taking the mean or flattening the embeddings. For the mean, I will not include the padding so that the word are not drowned out by padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentence_mean_emb'] = dataset['embedded_sentence'].apply(lambda x: x.mean(axis=0))\n",
    "dataset['flat_sentence'] = dataset['padded_sentence'].apply(lambda x: x.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Model training and validation\n",
    "\n",
    "As a first approach, the only feature I will give to the model is the embeddings of words in the sentence. Later, I can try adding more features.\n",
    "\n",
    "How many problem and solution sentences do we have in the training dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences = 4800\n",
      "Number of problem sentences = 1874\n",
      "Number of non-problem sentences = 2926\n",
      "Number of solution sentences = 800\n",
      "Number of non-solution sentences = 4000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of sentences = {len(dataset)}\")\n",
    "print(f\"Number of problem sentences = {sum(dataset['problem'] == 1)}\")\n",
    "print(f\"Number of non-problem sentences = {sum(dataset['problem'] == 0)}\")\n",
    "print(f\"Number of solution sentences = {sum(dataset['solution'] == 1)}\")\n",
    "print(f\"Number of non-solution sentences = {sum(dataset['solution'] == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a class imbalance, with almost twice as many non-problem compared to problem sentences, and five times as many non-solution compared to solution sentences. Because of this, I will use a ***Complement Naive Bayes*** model, which can handle class imbalances well, and generally works well with text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate the Naive Bayes model\n",
    "\n",
    "def test_nb_model(X, Y, embedding=''):\n",
    "    if type(dataset['sentence_mean_emb']) == pd.Series:\n",
    "        X = np.array(list(X))\n",
    "    Y = Y.values\n",
    "    # Split train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=5)\n",
    "    # MinMax scaling for Word2Vec because some values of embeddings are <0\n",
    "    if embedding != 'tf-idf':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    else:\n",
    "        scaler = ''\n",
    "    # Train Naive Bayes model\n",
    "    model = ComplementNB()\n",
    "    model.fit(X_train, Y_train) # Fit on training data\n",
    "    predicted = model.predict(X_test) # Predict on test data\n",
    "    # Calculate metrics - accuracy, precision, recall, and F1\n",
    "    accuracy = metrics.accuracy_score(predicted, Y_test)\n",
    "    print(\"Accuracy Score: \", accuracy)\n",
    "    precision = metrics.precision_score(predicted, Y_test)\n",
    "    print(f\"Precision: {precision}\")\n",
    "    recall = metrics.recall_score(predicted, Y_test)\n",
    "    print(f\"Recall: {recall}\")\n",
    "    f1 = metrics.f1_score(predicted, Y_test)\n",
    "    print(f\"F1-score: {f1}\")\n",
    "\n",
    "    # Could use below to plot ROC curve and determine best decision threshold\n",
    "    #predicted_prob = model.predict_proba(X_test)\n",
    "    #metrics.RocCurveDisplay.from_predictions(Y_test, predicted_prob[:,1])\n",
    "    return scaler, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train **problem** prediction models, and measure performance on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-IDF: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7991666666666667\n",
      "Precision: 0.6851441241685144\n",
      "Recall: 0.7573529411764706\n",
      "F1-score: 0.7194412107101281\n",
      "\n",
      "Using mean embedding: \n",
      "Accuracy Score:  0.6991666666666667\n",
      "Precision: 0.7006651884700665\n",
      "Recall: 0.5830258302583026\n",
      "F1-score: 0.6364551863041288\n",
      "\n",
      "Using flattened embedding: \n",
      "Accuracy Score:  0.6675\n",
      "Precision: 0.7361419068736141\n",
      "Recall: 0.5424836601307189\n",
      "F1-score: 0.6246472248353715\n"
     ]
    }
   ],
   "source": [
    "# Train & test using TF-IDF vectors.\n",
    "print('Using TF-IDF: ')\n",
    "pti_scaler, problem_tf_idf_model = test_nb_model(X_tfidf,\n",
    "                                                 dataset['problem'],\n",
    "                                                 embedding='tf-idf')\n",
    "\n",
    "# Train & test using mean over word embeddings for each sentence.\n",
    "print('\\nUsing mean embedding: ')\n",
    "pme_scaler, problem_mean_emb_model = test_nb_model(dataset['sentence_mean_emb'],\n",
    "                                                   dataset['problem'])\n",
    "\n",
    "# Train & test using flattened word embeddings for each sentence.\n",
    "print('\\nUsing flattened embedding: ')\n",
    "pfe_scaler, problem_flat_emb_model = test_nb_model(dataset['flat_sentence'],\n",
    "                                                   dataset['problem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train **solution** prediction models, and measure performance on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-IDF: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8508333333333333\n",
      "Precision: 0.4020100502512563\n",
      "Recall: 0.5714285714285714\n",
      "F1-score: 0.471976401179941\n",
      "\n",
      "Using mean embedding: \n",
      "Accuracy Score:  0.8375\n",
      "Precision: 0.7989949748743719\n",
      "Recall: 0.5063694267515924\n",
      "F1-score: 0.6198830409356725\n",
      "\n",
      "Using flattened embedding: \n",
      "Accuracy Score:  0.94\n",
      "Precision: 0.6934673366834171\n",
      "Recall: 0.9261744966442953\n",
      "F1-score: 0.7931034482758622\n"
     ]
    }
   ],
   "source": [
    "# Train & test using TF-IDF vectors.\n",
    "print('Using TF-IDF: ')\n",
    "sti_scaler, solution_tf_idf_model = test_nb_model(X_tfidf,\n",
    "                                                  dataset['solution'],\n",
    "                                                  embedding='tf-idf')\n",
    "\n",
    "# Train & test using mean over word embeddings for each sentence.\n",
    "print('\\nUsing mean embedding: ')\n",
    "sme_scaler, solution_mean_emb_model = test_nb_model(dataset['sentence_mean_emb'],\n",
    "                                                    dataset['solution'])\n",
    "\n",
    "# Train & test using flattened word embeddings for each sentence.\n",
    "print('\\nUsing flattened embedding: ')\n",
    "sfe_scaler, solution_flat_emb_model = test_nb_model(dataset['flat_sentence'],\n",
    "                                                    dataset['solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predicting **problems**, the model trained using TF-IDF vectors performs better than the models that use Word2Vec embeddings, achieving **accuracy=80%** and **F1=72%**.\n",
    "\n",
    "For predicting **solutions**, the model trained using TF-IDF vectors doesn't perform as well as the one that uses flattened Word2Vec embeddings, which achieves **accuracy=94%** and **F1=79%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - Adding parameters\n",
    "\n",
    "These models do a decent job of identifying sentences that contain problems and solutions. However, it is likely possible to improve their performance by including more features that I could extract from the text as predictors, as done in [1] - for example:\n",
    "- Polarity and subjectivity - using TextBlob as in the above section, but on the sentence.\n",
    "- Syntactic features - part-of-speech tags\n",
    "- Negation - if a sentence includes a negation word such as \"not\", \"n't\", \"no\", \"cannot\", this flips the meaning of the phrase. This could be included as a separate feature.\n",
    "- Transitive vs intransitive verbs - problems may be more likely to contain intransitive verbs (non-actions), whereas solutions may be more likely to contain transitive verbs (actions).\n",
    "- Presence of resultative adverbial modification (e.g. \"thus\", \"therefore\", \"consequently\")\n",
    "- Word embedding specifically of the head of the sentence.\n",
    "\n",
    "For now, I will implement two of these - polarity/subjectivity, and negation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity and subjectivity\n",
    "\n",
    "def run_textblob(sentence):\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentiment = TextBlob(sentence).sentiment\n",
    "    sentiment = pd.Series(sentiment, index=['polarity', 'subjectivity'])\n",
    "    # Rescake polarity to 0-1\n",
    "    sentiment['polarity'] = ((sentiment['polarity']+1)/2)\n",
    "    return sentiment\n",
    "\n",
    "dataset = pd.concat([dataset, dataset['sentences'].apply(run_textblob)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>ners</th>\n",
       "      <th>sentences</th>\n",
       "      <th>problem</th>\n",
       "      <th>solution</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>tfidf_text</th>\n",
       "      <th>embedded_sentence</th>\n",
       "      <th>padded_sentence</th>\n",
       "      <th>sentence_mean_emb</th>\n",
       "      <th>flat_sentence</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_extract_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[It, supports, rule, extraction, that, is, sen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[support, rule, extraction, sensitive, input, ...</td>\n",
       "      <td>sensitive training simianer rule feature simil...</td>\n",
       "      <td>[[-0.08984375, -0.14746094, -0.049560547, 0.00...</td>\n",
       "      <td>[[-0.08984375, -0.14746094, -0.049560547, 0.00...</td>\n",
       "      <td>[0.005493164, -0.03227539, 0.040148925, 0.0623...</td>\n",
       "      <td>[-0.08984375, -0.14746094, -0.049560547, 0.002...</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_extract_1</td>\n",
       "      <td>[[0, 3, ROOT-RIGHT], [6, 6, SIGNAL]]</td>\n",
       "      <td>[On, -, demand, extraction, can, be, slow, ,, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[demand, extraction, can, slow, phrase, base, ...</td>\n",
       "      <td>massive parallelization unit can purpose slow ...</td>\n",
       "      <td>[[-0.027709961, 0.0134887695, -0.03173828, 0.1...</td>\n",
       "      <td>[[-0.027709961, 0.0134887695, -0.03173828, 0.1...</td>\n",
       "      <td>[0.074298255, 0.06698418, 0.05682373, 0.082034...</td>\n",
       "      <td>[-0.027709961, 0.0134887695, -0.03173828, 0.15...</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_extract_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[He, et, al., (, 2013, ), demonstrated, orders...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[et, al, 2013, demonstrate, order, magnitude, ...</td>\n",
       "      <td>suffix al heart demand zhang exact extraction ...</td>\n",
       "      <td>[[-0.24902344, 0.22949219, 0.26953125, -0.1933...</td>\n",
       "      <td>[[-0.24902344, 0.22949219, 0.26953125, -0.1933...</td>\n",
       "      <td>[-0.023045858, 0.04418071, 0.06218465, 0.09068...</td>\n",
       "      <td>[-0.24902344, 0.22949219, 0.26953125, -0.19335...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_extract_1</td>\n",
       "      <td>[[30, 36, ROOT-RIGHT], [37, 39, SIGNAL], [40, ...</td>\n",
       "      <td>[However, ,, some, popular, translation, model...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[however, popular, translation, model, use, ``...</td>\n",
       "      <td>al use model work 2007 contiguous gpu limit ga...</td>\n",
       "      <td>[[0.15039062, 0.041259766, -0.06542969, 0.1020...</td>\n",
       "      <td>[[0.15039062, 0.041259766, -0.06542969, 0.1020...</td>\n",
       "      <td>[0.028714936, -0.0021846376, 0.039727572, 0.12...</td>\n",
       "      <td>[0.15039062, 0.041259766, -0.06542969, 0.10205...</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_extract_1</td>\n",
       "      <td>[[2, 2, ROOT-RIGHT], [3, 3, SIGNAL], [4, 17, C...</td>\n",
       "      <td>[Instead, ,, we, need, pattern, matching, and,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[instead, need, pattern, match, phrase, extrac...</td>\n",
       "      <td>variable lopez instead able gap pattern match ...</td>\n",
       "      <td>[[0.19726562, -0.0059814453, 0.22265625, 0.316...</td>\n",
       "      <td>[[0.19726562, -0.0059814453, 0.22265625, 0.316...</td>\n",
       "      <td>[0.062605634, 0.040715143, 0.008222139, 0.0845...</td>\n",
       "      <td>[0.19726562, -0.0059814453, 0.22265625, 0.3164...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_key                                               ners  \\\n",
       "0  train_extract_1                                                 []   \n",
       "1  train_extract_1               [[0, 3, ROOT-RIGHT], [6, 6, SIGNAL]]   \n",
       "2  train_extract_1                                                 []   \n",
       "3  train_extract_1  [[30, 36, ROOT-RIGHT], [37, 39, SIGNAL], [40, ...   \n",
       "4  train_extract_1  [[2, 2, ROOT-RIGHT], [3, 3, SIGNAL], [4, 17, C...   \n",
       "\n",
       "                                           sentences  problem  solution  \\\n",
       "0  [It, supports, rule, extraction, that, is, sen...        0         0   \n",
       "1  [On, -, demand, extraction, can, be, slow, ,, ...        1         0   \n",
       "2  [He, et, al., (, 2013, ), demonstrated, orders...        0         0   \n",
       "3  [However, ,, some, popular, translation, model...        1         0   \n",
       "4  [Instead, ,, we, need, pattern, matching, and,...        1         0   \n",
       "\n",
       "                                  processed_sentence  \\\n",
       "0  [support, rule, extraction, sensitive, input, ...   \n",
       "1  [demand, extraction, can, slow, phrase, base, ...   \n",
       "2  [et, al, 2013, demonstrate, order, magnitude, ...   \n",
       "3  [however, popular, translation, model, use, ``...   \n",
       "4  [instead, need, pattern, match, phrase, extrac...   \n",
       "\n",
       "                                          tfidf_text  \\\n",
       "0  sensitive training simianer rule feature simil...   \n",
       "1  massive parallelization unit can purpose slow ...   \n",
       "2  suffix al heart demand zhang exact extraction ...   \n",
       "3  al use model work 2007 contiguous gpu limit ga...   \n",
       "4  variable lopez instead able gap pattern match ...   \n",
       "\n",
       "                                   embedded_sentence  \\\n",
       "0  [[-0.08984375, -0.14746094, -0.049560547, 0.00...   \n",
       "1  [[-0.027709961, 0.0134887695, -0.03173828, 0.1...   \n",
       "2  [[-0.24902344, 0.22949219, 0.26953125, -0.1933...   \n",
       "3  [[0.15039062, 0.041259766, -0.06542969, 0.1020...   \n",
       "4  [[0.19726562, -0.0059814453, 0.22265625, 0.316...   \n",
       "\n",
       "                                     padded_sentence  \\\n",
       "0  [[-0.08984375, -0.14746094, -0.049560547, 0.00...   \n",
       "1  [[-0.027709961, 0.0134887695, -0.03173828, 0.1...   \n",
       "2  [[-0.24902344, 0.22949219, 0.26953125, -0.1933...   \n",
       "3  [[0.15039062, 0.041259766, -0.06542969, 0.1020...   \n",
       "4  [[0.19726562, -0.0059814453, 0.22265625, 0.316...   \n",
       "\n",
       "                                   sentence_mean_emb  \\\n",
       "0  [0.005493164, -0.03227539, 0.040148925, 0.0623...   \n",
       "1  [0.074298255, 0.06698418, 0.05682373, 0.082034...   \n",
       "2  [-0.023045858, 0.04418071, 0.06218465, 0.09068...   \n",
       "3  [0.028714936, -0.0021846376, 0.039727572, 0.12...   \n",
       "4  [0.062605634, 0.040715143, 0.008222139, 0.0845...   \n",
       "\n",
       "                                       flat_sentence  polarity  subjectivity  \\\n",
       "0  [-0.08984375, -0.14746094, -0.049560547, 0.002...  0.550000      0.900000   \n",
       "1  [-0.027709961, 0.0134887695, -0.03173828, 0.15...  0.458333      0.633333   \n",
       "2  [-0.24902344, 0.22949219, 0.26953125, -0.19335...  0.625000      0.250000   \n",
       "3  [0.15039062, 0.041259766, -0.06542969, 0.10205...  0.632143      0.521429   \n",
       "4  [0.19726562, -0.0059814453, 0.22265625, 0.3164...  0.750000      0.625000   \n",
       "\n",
       "   negation  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         1  \n",
       "4         0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negation: look for any of the below-defined negation words\n",
    "# in the lemmas of the sentence.\n",
    "\n",
    "negation_words = ['not', 'n\\'t', 'never', 'hardly', 'scarcely',\n",
    "                  'nobody', 'nothing', 'nowhere', 'contradict',\n",
    "                  'disavow', 'deny', 'repudiate', 'none']\n",
    "def check_negation(sentence):\n",
    "    sentence = [word.lower() for word in sentence]\n",
    "    contains_negation = 1-set(sentence).isdisjoint(set(negation_words))\n",
    "    return contains_negation\n",
    "\n",
    "dataset['negation'] = dataset['processed_sentence'].apply(check_negation)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the word embedding type that gave the best model performance when trained with no additional features - so TF-IDF for the problem model, and flattened Word2Vec for the solution model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem model with added features:\n",
      "Accuracy Score:  0.8166666666666667\n",
      "Precision: 0.6740576496674058\n",
      "Recall: 0.8063660477453581\n",
      "F1-score: 0.7342995169082125\n",
      "\n",
      "Solution model with added features:\n",
      "Accuracy Score:  0.9408333333333333\n",
      "Precision: 0.6984924623115578\n",
      "Recall: 0.9266666666666666\n",
      "F1-score: 0.7965616045845272\n"
     ]
    }
   ],
   "source": [
    "# Prepare input features for problem model\n",
    "X = X_tfidf\n",
    "added_features = np.array(dataset[['polarity','subjectivity','negation']])\n",
    "X = np.concatenate((X, added_features), axis=1)\n",
    "# Train & test problem model\n",
    "print(f\"Problem model with added features:\")\n",
    "afpm_scaler, added_feature_problem_model = test_nb_model(X, dataset['problem'], embedding='tf-idf')\n",
    "\n",
    "# Prepare input features for solution model\n",
    "X = np.array(list(dataset['flat_sentence']))\n",
    "X = np.concatenate((X, added_features), axis=1)\n",
    "# Train & test solution model\n",
    "print(f\"\\nSolution model with added features:\")\n",
    "afsm_scaler, added_feature_solution_model = test_nb_model(X, dataset['solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these features to the problem model led to a small increase in both accuracy (79->82%) and F1 (72->73%).\n",
    "\n",
    "Adding these features to the solution model increased performance by a very small amount - both accuracy (94->95%) and F1 (79->80%).\n",
    "\n",
    "One more thing I would have done with this model had I had more time would have been to ***tune the alpha parameter*** of the classifier to find the optimal value to maximise accuracy. In addition, perhaps other features from those listed above would improve the performance of these Naive Bayes models. However, I am likely to hit a limit of performance.\n",
    "\n",
    "Thus, instead of this, I will test a different kind of model entirely: a BERT model, which generally achieves excellent performance on language classification tasks. In the next section, I will set up and fine-tune a BERT model (`bert-base-uncased`) to classify problem vs non-problem sentences. I will then train a separate model for solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT classifiers\n",
    "\n",
    "#### 4.1 Sentence preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts for BERT\n",
    "\n",
    "# We need to use BERT's tokenizer, which adds special tokens\n",
    "# and performs slightly differently from the way these sentences are tokenized.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to detokenize tokenized sentences\n",
    "def detokenize(sent):\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(sent)\n",
    "    text = re.sub('\\s*,\\s*', ', ', text)\n",
    "    text = re.sub('\\s*\\.\\s*', '. ', text)\n",
    "    text = re.sub('\\s*\\?\\s*', '? ', text)\n",
    "    return text\n",
    "\n",
    "# Create function to preprocess sentences\n",
    "def preprocessing_for_bert(data, max_len=50):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        # Could try removing this preprocessing step\n",
    "        sent = preprocess_text(sent)\n",
    "        # If already tokenized, join tokens to create sentence,\n",
    "        # because we need to use BERT's tokenizer\n",
    "        if type(sent) == list:\n",
    "            sent = detokenize(sent)\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True            \n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sjwal\\dw-task\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification - BERT model with linear classification layer\n",
    "problem_bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Run on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "problem_bert_model.cuda()\n",
    "\n",
    "# Load Adam optimizer\n",
    "optimizer = AdamW(problem_bert_model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare data and train model\n",
    "def prepare_dataloader(X,Y):\n",
    "    inputs, masks = preprocessing_for_bert(X)\n",
    "    labels = torch.tensor(Y)\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=32)\n",
    "    return dataloader\n",
    "\n",
    "# Define function to calculate accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Define function to print out elapsed time in readable format\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fine-tune BERT model\n",
    "def train_bert(model, X, Y, epochs=4, seed_val=42):\n",
    "\n",
    "    # Train-validation split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.25, random_state=5)\n",
    "    \n",
    "    # Create DataLoader for train and validation sets\n",
    "    train_dataloader = prepare_dataloader(X_train, Y_train)\n",
    "    val_dataloader = prepare_dataloader(X_test, Y_test)\n",
    "\n",
    "    # Set learning schedule:\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # Create scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    # Seed parameters\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        ## Training\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print(f\"======== Epoch {epoch_i + 1} / {epochs} ========\")\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        # Activate training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time and report\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print(f\"  Batch {step}  of  {len(train_dataloader)}.    Elapsed: {elapsed}\")\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # Reset gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform forward pass, return loss\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters, take a step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate average loss over training data, and store\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        print(f\"Training epoch duration: {format_time(time.time() - t0)}\")\n",
    "\n",
    "        ## Validation\n",
    "        # Measure performance on validation set after each epoch.\n",
    "        t0 = time.time()\n",
    "        # Put in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        eval_accuracy = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in val_dataloader:\n",
    "            \n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack inputs from dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Don't store gradients\n",
    "            with torch.no_grad():\n",
    "                # Run forward pass, calculate logit predictions\n",
    "                outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask)\n",
    "            # Get the logits output by the model, and move to CPU\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            # Track batch number\n",
    "            nb_eval_steps += 1\n",
    "        \n",
    "        # Final accuracy for this run\n",
    "        print(f\"Accuracy: {eval_accuracy/nb_eval_steps}\")\n",
    "        print(f\"Evaluation duration: {format_time(time.time() - t0)}\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Training and validation\n",
    "\n",
    "I will first train/evaluate the model for identifying problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:10\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.4676823944380853\n",
      "Training epoch duration: 0:00:27\n",
      "Accuracy: 0.8988486842105263\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:09\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.26142644760223616\n",
      "Training epoch duration: 0:00:27\n",
      "Accuracy: 0.9078947368421053\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:10\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.17364918661460413\n",
      "Training epoch duration: 0:00:27\n",
      "Accuracy: 0.9161184210526315\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:09\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.1276896997495035\n",
      "Training epoch duration: 0:00:27\n",
      "Accuracy: 0.9185855263157895\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train and validate model on problem/non-problem sentences.\n",
    "\n",
    "X = dataset['sentences'].values\n",
    "Y = dataset['problem'].values\n",
    "\n",
    "problem_bert_model = train_bert(problem_bert_model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems to perform much better than the Naive Bayes model for predicting sentences containing a problem! (At least based on accuracy). Below, I will directly compare the performance of these models on a separate test dataset.\n",
    "\n",
    "Now I will perform the same fine-tuning procedure to produce a BERT model that can predict solutions, rather than problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:10\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.17263529101897657\n",
      "Training epoch duration: 0:00:27\n",
      "Accuracy: 0.9490131578947368\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:09\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.09150145124166018\n",
      "Training epoch duration: 0:00:26\n",
      "Accuracy: 0.9629934210526315\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:09\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.07253354352659884\n",
      "Training epoch duration: 0:00:27\n",
      "Accuracy: 0.9629934210526315\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch 40  of  113.    Elapsed: 0:00:09\n",
      "  Batch 80  of  113.    Elapsed: 0:00:19\n",
      "Average training loss: 0.05625335066125984\n",
      "Training epoch duration: 0:00:26\n",
      "Accuracy: 0.96875\n",
      "Evaluation duration: 0:00:03\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train and validate model on solution/non-solution sentences.\n",
    "# Load BertForSequenceClassification - BERT model with linear classification layer\n",
    "solution_bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Run on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "solution_bert_model.cuda()\n",
    "\n",
    "# Load Adam optimizer\n",
    "optimizer = AdamW(solution_bert_model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "X = dataset['sentences'].values\n",
    "Y = dataset['solution'].values\n",
    "\n",
    "solution_bert_model = train_bert(solution_bert_model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the problem model, this solution model performs better than the Naive Bayes model!\n",
    "\n",
    "Now I will test both the Naive Bayes and BERT models on a separate test dataset from the same source and directly compare their performance on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count problems and solutions in test dataset, and clip to max=1.\n",
    "\n",
    "test_dataset['problem'] = 0\n",
    "test_dataset['problem'] = test_dataset['ners'].apply(lambda x: sum([y.count('SIGNAL') for y in x]))\n",
    "test_dataset['solution'] = 0\n",
    "test_dataset['solution'] = test_dataset['ners'].apply(lambda x: sum([y.count('SOLUTION') for y in x]))\n",
    "\n",
    "test_dataset['problem'].clip(upper=1, inplace=True)\n",
    "test_dataset['solution'].clip(upper=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to run BERT model\n",
    "\n",
    "# Function to prepare the dataloader\n",
    "def prepare_prediction_dataloader(X):\n",
    "    inputs, masks = preprocessing_for_bert(X)\n",
    "    test_data = TensorDataset(inputs, masks)\n",
    "    sampler = SequentialSampler(test_data)\n",
    "    dataloader = DataLoader(test_data, sampler=sampler, batch_size=32)\n",
    "    return dataloader\n",
    "\n",
    "# Create function to get predictions from a BERT model\n",
    "def bert_predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Predict\n",
    "    for batch in dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack inputs\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        # Move logits to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        # Store predictions\n",
    "        predictions.append(logits)\n",
    "\n",
    "    # Flatten predictions\n",
    "    predictions = [item for sublist in predictions for item in sublist]\n",
    "    predictions = np.argmax(predictions, axis=1).flatten()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for problem predictions:\n",
      "           Naive Bayes      BERT\n",
      "accuracy      0.806667  0.918333\n",
      "precision     0.666667  0.870833\n",
      "recall        0.816327  0.920705\n",
      "F1            0.733945  0.895075\n",
      "\n",
      "Metrics for solution predictions:\n",
      "           Naive Bayes      BERT\n",
      "accuracy      0.966667  0.981667\n",
      "precision     0.840000  0.960000\n",
      "recall        0.954545  0.932039\n",
      "F1            0.893617  0.945813\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing, embed, and flatten\n",
    "test_dataset['preprocessed'] = test_dataset['sentences'].apply(preprocess_text)\n",
    "# TF-IDF\n",
    "test_dataset['tfidf_text'] = test_dataset['preprocessed'].apply(lambda text: \" \".join(set(text)))\n",
    "X_tfidf = tfidf_vect.transform(test_dataset['tfidf_text'])\n",
    "X_tfidf = X_tfidf.toarray()\n",
    "# Word2Vec\n",
    "test_dataset['embedded'] = test_dataset['preprocessed'].apply(embed_words)\n",
    "test_dataset['padded'] = test_dataset['embedded'].apply(pad_sentences)\n",
    "test_dataset['flat_emb'] = test_dataset['padded'].apply(lambda x: x.flatten())\n",
    "# Polarity/subjectivity\n",
    "dataset = pd.concat([test_dataset, test_dataset['sentences'].apply(run_textblob)], axis=1)\n",
    "# Negation\n",
    "dataset['negation'] = dataset['preprocessed'].apply(check_negation)\n",
    "# Add features to input vectors\n",
    "added_features = np.array(dataset[['polarity','subjectivity','negation']])\n",
    "\n",
    "# Create holder for results\n",
    "problem_predictions = pd.DataFrame({'Naive Bayes': [], 'BERT': []})\n",
    "solution_predictions = pd.DataFrame({'Naive Bayes': [], 'BERT': []})\n",
    "\n",
    "## Predict using Naive Bayes - use best-performing NB model for problems and solutions\n",
    "# Problems - use TF-IDF vectorizer and added features (added_feature_problem_model)\n",
    "# No MinMax scaling needed for TF-IDF\n",
    "X_prob = np.concatenate((X_tfidf, added_features), axis=1)\n",
    "y_pred = added_feature_problem_model.predict(X_prob)\n",
    "problem_predictions.at['accuracy', 'Naive Bayes'] = metrics.accuracy_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "problem_predictions.at['precision', 'Naive Bayes'] = metrics.precision_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "problem_predictions.at['recall', 'Naive Bayes'] = metrics.recall_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "problem_predictions.at['F1', 'Naive Bayes'] = metrics.f1_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "\n",
    "# Solutions - use flattened Word2Vec embeddings with added features\n",
    "X_sol = np.array(list(test_dataset['flat_emb']))\n",
    "X_sol = np.concatenate((X_sol, added_features), axis=1)\n",
    "X_sol = afsm_scaler.transform(X_sol)\n",
    "y_pred = added_feature_solution_model.predict(X_sol)\n",
    "solution_predictions.at['accuracy', 'Naive Bayes'] = metrics.accuracy_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "solution_predictions.at['precision', 'Naive Bayes'] = metrics.precision_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "solution_predictions.at['recall', 'Naive Bayes'] = metrics.recall_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "solution_predictions.at['F1', 'Naive Bayes'] = metrics.f1_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "\n",
    "## Predict using BERT\n",
    "# Problems\n",
    "X = test_dataset['sentences'].values\n",
    "test_dataloader = prepare_prediction_dataloader(X)\n",
    "y_pred = bert_predict(problem_bert_model, test_dataloader)\n",
    "problem_predictions.at['accuracy', 'BERT'] = metrics.accuracy_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "problem_predictions.at['precision', 'BERT'] = metrics.precision_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "problem_predictions.at['recall', 'BERT'] = metrics.recall_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "problem_predictions.at['F1', 'BERT'] = metrics.f1_score(\n",
    "    y_pred, test_dataset['problem'].values)\n",
    "\n",
    "# Solutions (input is the same as problem model)\n",
    "y_pred = bert_predict(solution_bert_model, test_dataloader)\n",
    "solution_predictions.at['accuracy', 'BERT'] = metrics.accuracy_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "solution_predictions.at['precision', 'BERT'] = metrics.precision_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "solution_predictions.at['recall', 'BERT'] = metrics.recall_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "solution_predictions.at['F1', 'BERT'] = metrics.f1_score(\n",
    "    y_pred, test_dataset['solution'].values)\n",
    "\n",
    "# Print results\n",
    "print('Metrics for problem predictions:')\n",
    "print(problem_predictions)\n",
    "print('\\nMetrics for solution predictions:')\n",
    "print(solution_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Key finding***\n",
    "For identifying both solutions and problems, the fine-tuned BERT models perform much better than the Naive Bayes models.\n",
    "\n",
    "In fact, these models performed ***better than the current state-of-the-art***. While the best-reported performance on this dataset is 90% and 86% accuracy for problems and solutions, respectively, these BERT models achieve a higher accuracy of 92% and 98% for problems and solutions, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the models for future inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sklearn models\n",
    "pickle.dump(added_feature_problem_model,\n",
    "            open('models/problem_nb_tdidf_model.sav', 'wb'))\n",
    "pickle.dump(added_feature_solution_model,\n",
    "            open('models/solution_nb_w2v_model.sav', 'wb'))\n",
    "pickle.dump(afsm_scaler,\n",
    "            open('models/solution_scaler.sav', 'wb'))\n",
    "\n",
    "# Save BERT models\n",
    "problem_bert_model.save_pretrained('models/problem_bert_model', include_optimizer=False)\n",
    "solution_bert_model.save_pretrained('models/solution_bert_model', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model usage on full article.\n",
    "\n",
    "Section under development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dw-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
